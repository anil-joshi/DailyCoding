{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Day 3**\n",
    "\n",
    "### Basics of Text Processing using NLTK (Contd.)\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Since we are trying to generate as much insights as we can from the unstructured text, its important to have the text in its simplest form. Stemming is a process to normalize words. It also helps convert various forms of words, used in various context to its stem. This may not give a grammatically correct or dictionary word. For instance, stem of words driving and drives is drive. There are multiple algorithms for stemming. In this one, I am only trying the Porter's Stemmer since it has lower error rate compared to many others. \n",
    "\n",
    "\n",
    "#### Lemmatization\n",
    "\n",
    "Lemmatization is a similar process to stem the words to its similar base. However, it doesn't just truncate the words, it provides a dictionary word. It is usually done without context and can result in a lower accuracy rate. Lemmatization also takes on part of speech values and has a lower processing speed compared to stemming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary packages\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do play they miss give so up. Words to up style of since world. We leaf to snug on no need. Way own uncommonly travelling now acceptance bed compliment solicitude. Dissimilar admiration so terminated no in contrasted it. Advantages entreaties mr he apartments do. Limits far yet turned highly repair parish talked six. Draw fond rank form nor the day eat. \n",
      "\n",
      "Necessary ye contented newspaper zealously breakfast he prevailed. Melancholy middletons yet understood decisively boy law she. Answer him easily are its barton little. Oh no though mother be things simple itself. Dashwood horrible he strictly on as. Home fine in so am good body this hope. \n",
      "\n",
      "Wrong do point avoid by fruit learn or in death. So passage however besides invited comfort elderly be me. Walls began of child civil am heard hoped my. Satisfied pretended mr on do determine by. Old post took and ask seen fact rich. Man entrance settling believed eat joy. Money as drift begin on to. Comparison up insipidity especially discovered me of decisively in surrounded. Points six way enough she its father. Folly sex downs tears ham green forty. \n",
      "\n",
      "Boy favourable day can introduced sentiments entreaties. Noisier carried of in warrant because. So mr plate seems cause chief widen first. Two differed husbands met screened his. Bed was form wife out ask draw. Wholly coming at we no enable. Offending sir delivered questions now new met. Acceptance she interested new boisterous day discretion celebrated. \n",
      "\n",
      "Remember outweigh do he desirous no cheerful. Do of doors water ye guest. We if prosperous comparison middletons at. Park we in lose like at no. An so to preferred convinced distrusts he determine. In musical me my placing clothes comfort pleased hearing. Any residence you satisfied and rapturous certainty two. Procured outweigh as outlived so so. On in bringing graceful proposal blessing of marriage outlived. Son rent face our loud near. \n",
      "\n",
      "Civility vicinity graceful is it at. Improve up at to on mention perhaps raising. Way building not get formerly her peculiar. Up uncommonly prosperous sentiments simplicity acceptance to so. Reasonable appearance companions oh by remarkably me invitation understood. Pursuit elderly ask perhaps all. \n",
      "\n",
      "Name were we at hope. Remainder household direction zealously the unwilling bed sex. Lose and gay ham sake met that. Stood her place one ten spoke yet. Head case knew ever set why over. Marianne returned of peculiar replying in moderate. Roused get enable garret estate old county. Entreaties you devonshire law dissimilar terminated. \n",
      "\n",
      "Sentiments two occasional affronting solicitude travelling and one contrasted. Fortune day out married parties. Happiness remainder joy but earnestly for off. Took sold add play may none him few. If as increasing contrasted entreaties be. Now summer who day looked our behind moment coming. Pain son rose more park way that. An stairs as be lovers uneasy. \n",
      "\n",
      "In entirely be to at settling felicity. Fruit two match men you seven share. Needed as or is enough points. Miles at smart ï»¿no marry whole linen mr. Income joy nor can wisdom summer. Extremely depending he gentleman improving intention rapturous as. \n",
      "\n",
      "Detract yet delight written farther his general. If in so bred at dare rose lose good. Feel and make two real miss use easy. Celebrated delightful an especially increasing instrument am. Indulgence contrasted sufficient to unpleasant in in insensible favourable. Latter remark hunted enough vulgar say man. Sitting hearted on it without me. \n",
      "3507\n"
     ]
    }
   ],
   "source": [
    "# Passing encoding = \"ISO-8859-1\" as I was having trouble loading the txt file.\n",
    "\n",
    "file = open(\"text.txt\", encoding = \"ISO-8859-1\")\n",
    "text = file.read()\n",
    "\n",
    "print (text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', 'play', 'they', 'miss', 'give', 'so', 'up', '.', 'Words', 'to']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using word_tokenize to generate the list of words from the above text.\n",
    "\n",
    "word = word_tokenize(text)\n",
    "word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', 'play', 'they', 'miss', 'give', 'so', 'up', '.', 'word', 'to']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Porter Stemmer for stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = []\n",
    "\n",
    "for w in word:\n",
    "    stemmed.append((stemmer.stem(w)))\n",
    "\n",
    "stemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', 'play', 'they', 'miss', 'give', 'so', 'up', '.', 'Words', 'to']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using WordNetLemmatizer for lemmatizing the words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = []\n",
    "\n",
    "for w in word:\n",
    "    lemmatized.append((lemmatizer.lemmatize(w)))\n",
    "\n",
    "lemmatized[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Do', 'VBP'),\n",
       " ('play', 'VB'),\n",
       " ('they', 'PRP'),\n",
       " ('miss', 'VBP'),\n",
       " ('give', 'VBP'),\n",
       " ('so', 'RB'),\n",
       " ('up', 'RB'),\n",
       " ('.', '.'),\n",
       " ('Words', 'NNS'),\n",
       " ('to', 'TO')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PoS for all the word\n",
    "\n",
    "tag = nltk.pos_tag(word)\n",
    "tag[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acceptance',\n",
       " 'add',\n",
       " 'admiration',\n",
       " 'advantages',\n",
       " 'affronting',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'answer']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# First, tokenizing the sentences from the above text\n",
    "sentence = sent_tokenize(text)\n",
    "\n",
    "# Creating an object for tfidf vectorizing later\n",
    "vectorizer = TfidfVectorizer(norm = None)\n",
    "\n",
    "# Generating output for TF IDF\n",
    "tfidf = vectorizer.fit_transform(sentence).toarray()\n",
    "\n",
    "vectorizer.get_feature_names()[:10]\n",
    "\n",
    "\n",
    "\n",
    "# To be continued ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
